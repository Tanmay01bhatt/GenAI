{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f18a9a",
   "metadata": {},
   "source": [
    "Creating the MCP Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44790588",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from mcp import ClientSession, StdioServerParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64231091",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define server parameters (adjust the path to your server script)\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[\"mcp-2.ipynb\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa314dc",
   "metadata": {},
   "source": [
    "LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c20a45",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import AnyMessage, add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0c028b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# LangGraph state definition\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35590955",
   "metadata": {},
   "source": [
    "Loading tools and configuring the language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd09f33",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69615ee",
   "metadata": {},
   "source": [
    "Defining the graph nodes and flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca12b96",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import tools_condition, ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f66bfa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "async def create_graph(session):\n",
    "    # Load tools from MCP server\n",
    "    tools = await load_mcp_tools(session)\n",
    "\n",
    "    # LLM configuration (system prompt can be added later)\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0, openai_api_key=\"{{OPENAI_API_KEY}}\")\n",
    "    llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "    # Prompt template with user/assistant chat only\n",
    "    prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant that uses tools to search Wikipedia.\"),\n",
    "        MessagesPlaceholder(\"messages\")\n",
    "    ])\n",
    "\n",
    "    chat_llm = prompt_template | llm_with_tools\n",
    "\n",
    "    # Define chat node\n",
    "    def chat_node(state: State) -> State:\n",
    "        state[\"messages\"] = chat_llm.invoke({\"messages\": state[\"messages\"]})\n",
    "        return state\n",
    "\n",
    "    # Build LangGraph with tool routing\n",
    "    graph = StateGraph(State)\n",
    "    graph.add_node(\"chat_node\", chat_node)\n",
    "    graph.add_node(\"tool_node\", ToolNode(tools=tools))\n",
    "    graph.add_edge(START, \"chat_node\")\n",
    "    graph.add_conditional_edges(\"chat_node\", tools_condition, {\n",
    "        \"tools\": \"tool_node\",\n",
    "        \"__end__\": END\n",
    "    })\n",
    "    graph.add_edge(\"tool_node\", \"chat_node\")\n",
    "\n",
    "    return graph.compile(checkpointer=MemorySaver())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aebff53",
   "metadata": {},
   "source": [
    "Running the agent with user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4738b4a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from mcp.client.stdio import stdio_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78735e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "async def main():\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "\n",
    "            agent = await create_graph(session)\n",
    "            print(\"Wikipedia MCP agent is ready.\")\n",
    "\n",
    "            while True:\n",
    "                user_input = input(\"\\nYou: \").strip()\n",
    "                if user_input.lower() in {\"exit\", \"quit\", \"q\"}:\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    response = await agent.ainvoke(\n",
    "                        {\"messages\": user_input},\n",
    "                        config={\"configurable\": {\"thread_id\": \"wiki-session\"}}\n",
    "                    )\n",
    "                    print(\"AI:\", response[\"messages\"][-1].content)\n",
    "                except Exception as e:\n",
    "                    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67648a87",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Run the asynchronous function\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
