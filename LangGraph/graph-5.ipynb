{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86bcee45",
   "metadata": {},
   "source": [
    "Parallel Tool Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743d6ea",
   "metadata": {},
   "source": [
    "LangGraph can branch into multiple paths, execute them simultaneously, and rejoin the results into one coherent response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa59a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c2039",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#state\n",
    "class MyMessagesState(MessagesState):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6271082",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75dead",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def tool_calling_llm(state: MyMessagesState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c06a4a",
   "metadata": {},
   "source": [
    "AIMessage is returned from a chat model as a response to a prompt. \n",
    "\n",
    "This message represents the output of the model and consists of both the raw output as returned by the model together standardized fields (e.g., tool calls, usage metadata) added by the LangChain framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9e59d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31f6dd9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def weather_paris(state: MyMessagesState):\n",
    "    text = get_weather(\"Paris\")\n",
    "    return {\"messages\": [AIMessage(content=text)]}\n",
    "\n",
    "def weather_london(state: MyMessagesState):\n",
    "    text = get_weather(\"London\")\n",
    "    return {\"messages\": [AIMessage(content=text)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe1fc7",
   "metadata": {},
   "source": [
    "If both are mentioned, it returns both. And if neither city is present, we fall back to Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf109af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def parallel_weather_condition(state: MyMessagesState):\n",
    "    last_human = \"\"\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if getattr(msg, \"type\", \"\") == \"human\":\n",
    "            last_human = (msg.content or \"\").lower()\n",
    "            break\n",
    "\n",
    "    targets = []\n",
    "    if \"paris\" in last_human:\n",
    "        targets.append(\"weather_paris\")\n",
    "    if \"london\" in last_human:\n",
    "        targets.append(\"weather_london\")\n",
    "\n",
    "    if not targets:\n",
    "        targets = [\"weather_paris\"]\n",
    "\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5763aa13",
   "metadata": {},
   "source": [
    "Join Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a757264e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def combine_weather(state: MyMessagesState):\n",
    "    lines = []\n",
    "    for m in state[\"messages\"]:\n",
    "        if isinstance(m, AIMessage) and isinstance(m.content, str) and \"The weather in\" in m.content:\n",
    "            lines.append(m.content)\n",
    "\n",
    "    combined = \" | \".join(lines) if lines else \"No weather data found.\"\n",
    "    return {\"messages\": [AIMessage(content=f\"Combined: {combined}\")]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65353483",
   "metadata": {},
   "source": [
    "Graph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e2c7e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caca5af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "builder = StateGraph(MyMessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35b144",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "builder.add_node(\"weather_paris\", weather_paris)\n",
    "builder.add_node(\"weather_london\", weather_london)\n",
    "builder.add_node(\"combine_weather\", combine_weather)\n",
    "\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_conditional_edges(\"tool_calling_llm\", parallel_weather_condition)\n",
    "builder.add_edge(\"weather_paris\", \"combine_weather\")\n",
    "builder.add_edge(\"weather_london\", \"combine_weather\")\n",
    "builder.add_edge(\"combine_weather\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e54e826",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a0be4",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de164418",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Case 1: both cities mentioned\n",
    "print(\"\\n=== Case 1: both cities mentioned ===\")\n",
    "messages = graph.invoke({\"messages\": [HumanMessage(content=\"What's the weather in Paris and London?\")]})\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()\n",
    "\n",
    "# Case 2: one city mentioned\n",
    "print(\"\\n=== Case 2: one city mentioned ===\")\n",
    "messages = graph.invoke({\"messages\": [HumanMessage(content=\"What's the weather in Paris?\")]})\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()\n",
    "\n",
    "# Case 3: no city mentioned (triggers fallback)\n",
    "print(\"\\n=== Case 3: no city mentioned (triggers fallback) ===\")\n",
    "messages = graph.invoke({\"messages\": [HumanMessage(content=\"Weather please\")]})\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
