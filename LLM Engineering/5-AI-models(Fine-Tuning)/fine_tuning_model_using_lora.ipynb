{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Yor9lXPGd0I"
      },
      "source": [
        "## Hugging Face CLI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "pip3 install transformers==4.44.1\n",
        "pip3 install accelerate\n",
        "pip3 install bitsandbytes==0.43.3\n",
        "pip3 install datasets==2.21.0\n",
        "pip3 install trl==0.9.6\n",
        "pip3 install peft==0.12.0\n",
        "!pip install -U \"huggingface_hub[cli]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Lcs_LdgWf55"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "print(\"Enter you Hugging Face token:\")\n",
        "TOKEN = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUflJA8lWhjW"
      },
      "outputs": [],
      "source": [
        "!git config --global credential.helper store\n",
        "!huggingface-cli login --token $TOKEN --add-to-git-credential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHrP3mjXjOmb"
      },
      "source": [
        "\n",
        "\n",
        "## Import the modules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBOE-h8sbrNK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Suppressing “INFO” and “WARNING” messages by setting the verbosity of the Transformers library.\n",
        "from transformers import logging\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "# Suppressing Python warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPiHEIuQve_T"
      },
      "source": [
        "## Load the model with 8-bit Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcJCluOlviMe"
      },
      "outputs": [],
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2EZhNQ66EAd"
      },
      "outputs": [],
      "source": [
        "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "quantized_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                    quantization_config = bnb_config,\n",
        "                    device_map = \"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcqAQxxkvLgz"
      },
      "source": [
        "## Inference of the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nd8GllBE2o4x"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "input = tokenizer(\"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\", return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "response = quantized_model.generate(**input, max_new_tokens = 100)\n",
        "print(tokenizer.batch_decode(response, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgSbWKmSu85Z"
      },
      "source": [
        "## Preprocessing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIr-HPlCrlZQ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = \"openai/gsm8k\"\n",
        "data = load_dataset(dataset, 'main')\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data = data.map(lambda samples: tokenizer(samples[\"question\"], samples[\"answer\"], truncation=True, padding=\"max_length\", max_length=100), batched=True)\n",
        "train_samples = data[\"train\"].select(range(400))\n",
        "\n",
        "display(train_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn4mMm46vIyG"
      },
      "outputs": [],
      "source": [
        "print(train_samples[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujH774PizZB7"
      },
      "source": [
        "## Training the model on dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68lRfcylzI2C"
      },
      "source": [
        "### LoRA configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_zTelMQMpRC"
      },
      "outputs": [],
      "source": [
        "import peft\n",
        "from peft import LoraConfig\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmjSGeTczOGx"
      },
      "source": [
        "### Setting the training arguments\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtesQqcJvlfw"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "import os\n",
        "\n",
        "working_dir = './'\n",
        "output_directory = os.path.join(working_dir, \"lora\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = output_directory,\n",
        "    auto_find_batch_size = True,\n",
        "    learning_rate = 3e-4,\n",
        "    num_train_epochs=5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkDVd6hnzkXc"
      },
      "source": [
        "### Setting the trainer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mju04eFSvd-P"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from trl import SFTTrainer\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = quantized_model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_samples,\n",
        "    peft_config = lora_config, tokenizer = tokenizer,\n",
        "    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7-ynfiFz52u"
      },
      "source": [
        "### Training the model\n",
        "\n",
        "It takes significant amount of time to train the model and we have a limited session time of Jupyter notebook on our platform. You can uncomment the code and execute on GPU enable machine to see the response.\n",
        "\n",
        "We have already trained the model for you. We will load the saved model later in the code for inference of fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "6FXZT5SvvyQN",
        "outputId": "dc6d66a6-efeb-4a62-d983-2e97d55f3204"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 36:47, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.273000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.862500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=1.06775, metrics={'train_runtime': 2209.425, 'train_samples_per_second': 0.905, 'train_steps_per_second': 0.453, 'total_flos': 9014088499200000.0, 'train_loss': 1.06775, 'epoch': 5.0})"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAJUMgtL8iIR"
      },
      "outputs": [],
      "source": [
        "# Save the model.\n",
        "model_path = os.path.join(output_directory, f\"lora_model\")\n",
        "\n",
        "trainer.model.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RctVsPyXwAhh"
      },
      "outputs": [],
      "source": [
        "#We are going to clean some variables to avoid memory problems\n",
        "import gc\n",
        "import torch\n",
        "del quantized_model\n",
        "del trainer\n",
        "del train_samples\n",
        "del data\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8PpfCukySNI"
      },
      "source": [
        "## Load the fine-tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF0ArwFewESp"
      },
      "outputs": [],
      "source": [
        "model_path = \"/trained_models/lora/lora_model\"\n",
        "\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        ")\n",
        "\n",
        "loaded_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "                                        model_path,\n",
        "                                        quantization_config = bnb_config,\n",
        "                                        device_map = 'auto')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g_YtP4uynPQ"
      },
      "source": [
        "## Inference of the fine-tuned model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N5IW31r0PTQ",
        "outputId": "19ddcbce-ac02-4544-904a-b6653af254b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?The number of clips Natalia sold in May is 48/2 = <<48/2=24>>24 clips.\\nThe total number of clips Natalia sold in April and May is 48+24 = <<48+24=72>>72 clips.\\n#### 72 ####\\n#### 72 ####\\n#### 72 ####\\n#### 72 ####\\n#### 72 ####\\n#### 72 ####\\n#### 72 ####\\n#### 72 ####\\n#### 72 ####\\n####']\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "input = tokenizer(\"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\", return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "response = loaded_model.generate(**input, max_new_tokens = 100)\n",
        "print(tokenizer.batch_decode(response, skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
