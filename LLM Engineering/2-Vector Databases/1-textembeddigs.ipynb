{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rH1UpepYqtr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load dataset from CSV file\n",
        "df = pd.read_csv(\"/usr/local/datasetsDir/text-dataset/job_title_des.csv\")\n",
        "\n",
        "# Read only the first twenty job descriptions\n",
        "df = df.head(20)\n",
        "\n",
        "# Preprocess text data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token not in string.punctuation]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['processed_description'] = df['Job Description'].apply(preprocess)\n",
        "\n",
        "# Load pretrained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to generate BERT embedding for a text using [CLS] token\n",
        "def generate_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Extract [CLS] token embedding\n",
        "    return cls_embedding.numpy()\n",
        "\n",
        "# Function for semantic search\n",
        "def semantic_search(query, top_n=3):\n",
        "    query_embedding = generate_embedding(query)\n",
        "    similarities = []\n",
        "    for idx, row in df.iterrows():\n",
        "        desc_embedding = generate_embedding(row['processed_description'])\n",
        "        similarity = cosine_similarity(query_embedding, desc_embedding)[0][0]\n",
        "        similarities.append((row['Job Title'], similarity, row['Job Description']))\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:top_n]\n",
        "\n",
        "# Example usage\n",
        "query = \"python developer with experience in web development\"\n",
        "results = semantic_search(query)\n",
        "for title, similarity, description in results:\n",
        "    print(f\"Title: {title}, Similarity: {similarity:.2f}\")"
      ]
    }
  ]
}