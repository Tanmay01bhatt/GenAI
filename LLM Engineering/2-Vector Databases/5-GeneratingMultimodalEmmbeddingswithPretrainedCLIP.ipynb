{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xOQF43yVZss"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the model and preprocess function from CLIP\n",
        "model, preprocess = clip.load(\"ViT-B/32\")\n",
        "model.eval()\n",
        "\n",
        "# Path configurations\n",
        "images_folder = '/usr/local/datasetsDir/images-and-descriptions/data/images'\n",
        "csv_file = '/usr/local/datasetsDir/images-and-descriptions/data/image_descriptions.csv'\n",
        "\n",
        "# Load the CSV file with image IDs and descriptions\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Prepare lists for image paths and descriptions\n",
        "image_paths = []\n",
        "descriptions = []\n",
        "\n",
        "# Iterate through the CSV file to get image paths and corresponding descriptions\n",
        "for _, row in df.iterrows():\n",
        "    image_id = str(row[0])  # Ensure that the ID is a string for matching\n",
        "    description = row[1]\n",
        "    # Find the image file corresponding to the image_id\n",
        "    for file_name in os.listdir(images_folder):\n",
        "        if file_name.startswith(f\"{image_id}_\") and file_name.endswith('.png'):\n",
        "            image_path = os.path.join(images_folder, file_name)\n",
        "            image_paths.append(image_path)\n",
        "            descriptions.append(description)\n",
        "            break\n",
        "\n",
        "# Function to preprocess images and text descriptions\n",
        "def preprocess_data(image_paths, descriptions):\n",
        "    images = [preprocess(Image.open(image_path)) for image_path in image_paths]\n",
        "    text_inputs = [clip.tokenize(description, truncate=True) for description in descriptions]\n",
        "    return torch.stack(images), torch.cat(text_inputs)\n",
        "\n",
        "# Preprocess images and descriptions\n",
        "images, text_inputs = preprocess_data(image_paths, descriptions)\n",
        "\n",
        "# Generate embeddings\n",
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(images)\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "# Normalize the features\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "# Print embeddings\n",
        "print(\"Image embeddings: \", image_features)\n",
        "print(\"Text embeddings: \", text_features)\n",
        "\n",
        "# Save the embeddings and metadata for later use\n",
        "torch.save({\n",
        "    'image_features': image_features,\n",
        "    'text_features': text_features,\n",
        "    'image_paths': image_paths,\n",
        "    'descriptions': descriptions\n",
        "}, '/usr/local/datasetsDir/images-and-descriptions/embeddings.pt')\n",
        "\n",
        "print(\"Embeddings generated and saved successfully.\")"
      ]
    }
  ]
}